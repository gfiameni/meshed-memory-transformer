{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from data import ImageDetectionsField, TextField, RawField\n",
    "from data import COCO, DataLoader\n",
    "import evaluation\n",
    "from evaluation import PTBTokenizer, Cider\n",
    "from models.transformer import Transformer, LinearEncoder, Decoder\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import NLLLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse, os, pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from shutil import copyfile\n",
    "\n",
    "import horovod.torch as hvd\n",
    "import torch.multiprocessing as mp\n",
    "from apex import amp\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, dataloader, loss_fn, text_field):\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    running_loss = .0\n",
    "    with tqdm(desc='Epoch %d - validation' % e, unit='it', total=len(dataloader)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for it, (detections, captions) in enumerate(dataloader):\n",
    "                detections, captions = detections.to(device), captions.to(device)\n",
    "                out = model(detections, captions)\n",
    "                captions = captions[:, 1:].contiguous()\n",
    "                out = out[:, :-1].contiguous()\n",
    "                loss = loss_fn(out.view(-1, len(text_field.vocab)), captions.view(-1))\n",
    "                this_loss = loss.item()\n",
    "                running_loss += this_loss\n",
    "\n",
    "                pbar.set_postfix(loss=running_loss / (it + 1))\n",
    "                pbar.update()\n",
    "\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def evaluate_metrics(model, dataloader, text_field):\n",
    "    import itertools\n",
    "    model.eval()\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "    with tqdm(desc='Epoch %d - evaluation' % e, unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, (images, caps_gt) in enumerate(iter(dataloader)):\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "                out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1)\n",
    "            caps_gen = text_field.decode(out, join_words=False)\n",
    "            for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "                gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                gen['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                gts['%d_%d' % (it, i)] = gts_i\n",
    "            pbar.update()\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "    scores, _ = evaluation.compute_scores(gts, gen)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def train_xe(model, dataloader, optim, text_field):\n",
    "    # Training with cross-entropy\n",
    "    model.train()\n",
    "    # scheduler.step()\n",
    "    running_loss = .0\n",
    "    with tqdm(desc='Epoch %d - train' % e, unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, (detections, captions) in enumerate(dataloader):\n",
    "            detections, captions = detections.to(device), captions.to(device)\n",
    "            out = model(detections, captions)\n",
    "\n",
    "            if args.use_amp:\n",
    "                optim.synchronize()\n",
    "                \n",
    "            optim.zero_grad()\n",
    "            captions_gt = captions[:, 1:].contiguous()\n",
    "            out = out[:, :-1].contiguous()\n",
    "            loss = loss_fn(out.view(-1, len(text_field.vocab)), captions_gt.view(-1))\n",
    "\n",
    "            if args.use_amp:\n",
    "                with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if args.use_amp:\n",
    "                optim.skip_synchronize()\n",
    "            else:\n",
    "                optim.step()\n",
    "\n",
    "            this_loss = loss.item()\n",
    "            running_loss += this_loss\n",
    "\n",
    "            pbar.set_postfix(loss=running_loss / (it + 1))\n",
    "            pbar.update()\n",
    "            scheduler.step()\n",
    "\n",
    "    loss = running_loss / len(dataloader)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_scst(model, dataloader, optim, cider, text_field):\n",
    "    # Training with self-critical\n",
    "    tokenizer_pool = multiprocessing.Pool()\n",
    "    running_reward = .0\n",
    "    running_reward_baseline = .0\n",
    "    model.train()\n",
    "    running_loss = .0\n",
    "    seq_len = 20\n",
    "    beam_size = 5\n",
    "\n",
    "    with tqdm(desc='Epoch %d - train' % e, unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, (detections, caps_gt) in enumerate(dataloader):\n",
    "            detections = detections.to(device)\n",
    "            outs, log_probs = model.beam_search(detections, seq_len, text_field.vocab.stoi['<eos>'],\n",
    "                                                beam_size, out_size=beam_size)\n",
    "\n",
    "            if args.use_amp:\n",
    "                optim.synchronize()\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Rewards\n",
    "            caps_gen = text_field.decode(outs.view(-1, seq_len))\n",
    "            caps_gt = list(itertools.chain(*([c, ] * beam_size for c in caps_gt)))\n",
    "            caps_gen, caps_gt = tokenizer_pool.map(evaluation.PTBTokenizer.tokenize, [caps_gen, caps_gt])\n",
    "            reward = cider.compute_score(caps_gt, caps_gen)[1].astype(np.float32)\n",
    "            reward = torch.from_numpy(reward).to(device).view(detections.shape[0], beam_size)\n",
    "            reward_baseline = torch.mean(reward, -1, keepdim=True)\n",
    "            loss = -torch.mean(log_probs, -1) * (reward - reward_baseline)\n",
    "\n",
    "            loss = loss.mean()\n",
    "\n",
    "            if args.use_amp:\n",
    "                with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if args.use_amp:\n",
    "                optim.skip_synchronize()\n",
    "            else:\n",
    "                optim.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_reward += reward.mean().item()\n",
    "            running_reward_baseline += reward_baseline.mean().item()\n",
    "            pbar.set_postfix(loss=running_loss / (it + 1), reward=running_reward / (it + 1),\n",
    "                             reward_baseline=running_reward_baseline / (it + 1))\n",
    "            pbar.update()\n",
    "\n",
    "    loss = running_loss / len(dataloader)\n",
    "    reward = running_reward / len(dataloader)\n",
    "    reward_baseline = running_reward_baseline / len(dataloader)\n",
    "    return loss, reward, reward_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'exp_name': 'transformer', \n",
    "       'batch_size' : 200 , \n",
    "       'workers' : 2, \n",
    "       'head' : 8, \n",
    "       'warmup' : 10000, \n",
    "       'resume_last' : False, 'resume_best' : False,\n",
    "       'features_path' : './data/coco_detections.hdf5',\n",
    "       'annotation_folder' : './data/annotations/',\n",
    "       'logs_folder' : './tensorboard_logs',\n",
    "       'N_enc' : 1,\n",
    "       'N_dec' : 1,\n",
    "       'use_amp' : True,\n",
    "       'cuda' : True\n",
    "        }\n",
    "\n",
    "args = objectview(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Training\n"
     ]
    }
   ],
   "source": [
    "print('Transformer Training')\n",
    "\n",
    "writer = SummaryWriter(log_dir=os.path.join(args.logs_folder, args.exp_name))\n",
    "\n",
    "# Pipeline for image regions\n",
    "image_field = ImageDetectionsField(detections_path=args.features_path, max_detections=50, load_in_tmp=False)\n",
    "\n",
    "# Pipeline for text\n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy',\n",
    "                   remove_punctuation=True, nopoints=False)\n",
    "\n",
    "# Horovod: initialize library.\n",
    "hvd.init()\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if args.cuda:\n",
    "    # Horovod: pin GPU to local rank.\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# Horovod: limit # of CPU threads to be used per worker.\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "kwargs = {'pin_memory': True} if args.cuda else {}\n",
    "# When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n",
    "# issues with Infiniband implementations that are not fork-safe\n",
    "if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
    "    mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
    "    kwargs['multiprocessing_context'] = 'forkserver'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = COCO(image_field, text_field, 'coco/images/', args.annotation_folder, args.annotation_folder)\n",
    "train_dataset, val_dataset, test_dataset = dataset.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data.dataset.PairedDataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('vocab_%s.pkl' % args.exp_name):\n",
    "    print(\"Building vocabulary\")\n",
    "    text_field.build_vocab(train_dataset, val_dataset, min_freq=5)\n",
    "    pickle.dump(text_field.vocab, open('vocab_%s.pkl' % args.exp_name, 'wb'))\n",
    "else:\n",
    "    text_field.vocab = pickle.load(open('vocab_%s.pkl' % args.exp_name, 'rb'))\n",
    "\n",
    "# Model and dataloaders\n",
    "encoder = LinearEncoder(args.N_enc, 0)\n",
    "decoder = Decoder(len(text_field.vocab), 54, args.N_dec, text_field.vocab.stoi['<pad>'])\n",
    "model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)\n",
    "\n",
    "dict_dataset_train = train_dataset.image_dictionary({'image': image_field, 'text': RawField()})\n",
    "ref_caps_train = list(train_dataset.text)\n",
    "cider_train = Cider(PTBTokenizer.tokenize(ref_caps_train))\n",
    "dict_dataset_val = val_dataset.image_dictionary({'image': image_field, 'text': RawField()})\n",
    "dict_dataset_test = test_dataset.image_dictionary({'image': image_field, 'text': RawField()})\n",
    "\n",
    "\n",
    "def lambda_lr(s):\n",
    "    warm_up = args.warmup\n",
    "    s += 1\n",
    "    return (model.d_model ** -.5) * min(s ** -.5, s * warm_up ** -1.5)\n",
    "\n",
    "\n",
    "# Initial conditions\n",
    "optim = Adam(model.parameters(), lr=1, betas=(0.9, 0.98))\n",
    "\n",
    "# Horovod: wrap optimizer with DistributedOptimizer.\n",
    "optim = hvd.DistributedOptimizer(optim, named_parameters=model.named_parameters(), )\n",
    "\n",
    "if args.use_amp:\n",
    "model, optim = amp.initialize(model, optim, opt_level=\"O1\")\n",
    "\n",
    "scheduler = LambdaLR(optim, lambda_lr)\n",
    "\n",
    "# Horovod: (optional) compression algorithm.\n",
    "# compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n",
    "\n",
    "\n",
    "loss_fn = NLLLoss(ignore_index=text_field.vocab.stoi['<pad>'])\n",
    "use_rl = False\n",
    "best_cider = .0\n",
    "patience = 0\n",
    "start_epoch = 0\n",
    "\n",
    "if args.resume_last or args.resume_best:\n",
    "if args.resume_last:\n",
    "    fname = 'saved_models/%s_last.pth' % args.exp_name\n",
    "else:\n",
    "    fname = 'saved_models/%s_best.pth' % args.exp_name\n",
    "\n",
    "if os.path.exists(fname):\n",
    "    data = torch.load(fname)\n",
    "    torch.set_rng_state(data['torch_rng_state'])\n",
    "    torch.cuda.set_rng_state(data['cuda_rng_state'])\n",
    "    np.random.set_state(data['numpy_rng_state'])\n",
    "    random.setstate(data['random_rng_state'])\n",
    "    model.load_state_dict(data['state_dict'], strict=False)\n",
    "    optim.load_state_dict(data['optimizer'])\n",
    "    scheduler.load_state_dict(data['scheduler'])\n",
    "    start_epoch = data['epoch'] + 1\n",
    "    best_cider = data['best_cider']\n",
    "    patience = data['patience']\n",
    "    use_rl = data['use_rl']\n",
    "    print('Resuming from epoch %d, validation loss %f, and best cider %f' % (\n",
    "        data['epoch'], data['val_loss'], data['best_cider']))\n",
    "\n",
    "\n",
    "# Horovod Distribute Sampler\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "\n",
    "dict_train_sampler = torch.utils.data.distributed.DistributedSampler(dict_dataset_train, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "\n",
    "dict_val_sampler = torch.utils.data.distributed.DistributedSampler(dict_dataset_val, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "\n",
    "dict_test_sampler = torch.utils.data.distributed.DistributedSampler(dict_dataset_test, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "\n",
    "# Broadcast parameters from rank 0 to all other processes.\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "hvd.broadcast_optimizer_state(optim, root_rank=0)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, drop_last=True, sampler=train_sampler, **kwargs)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, sampler=val_sampler, **kwargs)\n",
    "dict_dataloader_train = DataLoader(dict_dataset_train, batch_size=args.batch_size // 5, shuffle=False, num_workers=args.workers, sampler=dict_train_sampler, **kwargs)\n",
    "dict_dataloader_val = DataLoader(dict_dataset_val, batch_size=args.batch_size // 5, sampler=dict_val_sampler, **kwargs)\n",
    "dict_dataloader_test = DataLoader(dict_dataset_test, batch_size=args.batch_size // 5, sampler=dict_test_sampler, **kwargs)\n",
    "\n",
    "print(\"Training starts\")\n",
    "for e in range(start_epoch, start_epoch + 100):\n",
    "\n",
    "if not use_rl:\n",
    "    train_loss = train_xe(model, dataloader_train, optim, text_field)\n",
    "    writer.add_scalar('data/train_loss', train_loss, e)\n",
    "else:\n",
    "    hvd.broadcast_optimizer_state(optim, root_rank=0)\n",
    "\n",
    "    train_loss, reward, reward_baseline = train_scst(model, dict_dataloader_train, optim, cider_train, text_field)\n",
    "    writer.add_scalar('data/train_loss', train_loss, e)\n",
    "    writer.add_scalar('data/reward', reward, e)\n",
    "    writer.add_scalar('data/reward_baseline', reward_baseline, e)\n",
    "\n",
    "# Validation loss\n",
    "val_loss = evaluate_loss(model, dataloader_val, loss_fn, text_field)\n",
    "writer.add_scalar('data/val_loss', val_loss, e)\n",
    "\n",
    "# Validation scores\n",
    "scores = evaluate_metrics(model, dict_dataloader_val, text_field)\n",
    "print(\"Validation scores\", scores)\n",
    "val_cider = scores['CIDEr']\n",
    "writer.add_scalar('data/val_cider', val_cider, e)\n",
    "writer.add_scalar('data/val_bleu1', scores['BLEU'][0], e)\n",
    "writer.add_scalar('data/val_bleu4', scores['BLEU'][3], e)\n",
    "writer.add_scalar('data/val_meteor', scores['METEOR'], e)\n",
    "writer.add_scalar('data/val_rouge', scores['ROUGE'], e)\n",
    "\n",
    "# Test scores\n",
    "scores = evaluate_metrics(model, dict_dataloader_test, text_field)\n",
    "print(\"Test scores\", scores)\n",
    "writer.add_scalar('data/test_cider', scores['CIDEr'], e)\n",
    "writer.add_scalar('data/test_bleu1', scores['BLEU'][0], e)\n",
    "writer.add_scalar('data/test_bleu4', scores['BLEU'][3], e)\n",
    "writer.add_scalar('data/test_meteor', scores['METEOR'], e)\n",
    "writer.add_scalar('data/test_rouge', scores['ROUGE'], e)\n",
    "\n",
    "# Prepare for next epoch\n",
    "best = False\n",
    "if val_cider >= best_cider:\n",
    "    best_cider = val_cider\n",
    "    patience = 0\n",
    "    best = True\n",
    "else:\n",
    "    patience += 1\n",
    "\n",
    "switch_to_rl = False\n",
    "exit_train = False\n",
    "if patience == 5:\n",
    "    if not use_rl:\n",
    "        use_rl = True\n",
    "        switch_to_rl = True\n",
    "        patience = 0\n",
    "        optim = Adam(model.parameters(), lr=5e-6)\n",
    "        print(\"Switching to RL\")\n",
    "    else:\n",
    "        print('patience reached.')\n",
    "        exit_train = True\n",
    "\n",
    "if switch_to_rl and not best:\n",
    "    data = torch.load('saved_models/%s_best.pth' % args.exp_name)\n",
    "    torch.set_rng_state(data['torch_rng_state'])\n",
    "    torch.cuda.set_rng_state(data['cuda_rng_state'])\n",
    "    np.random.set_state(data['numpy_rng_state'])\n",
    "    random.setstate(data['random_rng_state'])\n",
    "    model.load_state_dict(data['state_dict'])\n",
    "    print('Resuming from epoch %d, validation loss %f, and best cider %f' % (\n",
    "        data['epoch'], data['val_loss'], data['best_cider']))\n",
    "\n",
    "torch.save({\n",
    "    'torch_rng_state': torch.get_rng_state(),\n",
    "    'cuda_rng_state': torch.cuda.get_rng_state(),\n",
    "    'numpy_rng_state': np.random.get_state(),\n",
    "    'random_rng_state': random.getstate(),\n",
    "    'epoch': e,\n",
    "    'val_loss': val_loss,\n",
    "    'val_cider': val_cider,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optim.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'patience': patience,\n",
    "    'best_cider': best_cider,\n",
    "    'use_rl': use_rl,\n",
    "}, 'saved_models/%s_last.pth' % args.exp_name)\n",
    "\n",
    "if best:\n",
    "    copyfile('saved_models/%s_last.pth' % args.exp_name, 'saved_models/%s_best.pth' % args.exp_name)\n",
    "\n",
    "if exit_train:\n",
    "    writer.close()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
